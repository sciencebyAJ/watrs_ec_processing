{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sciencebyAJ/watrs_ec_processing/blob/main/WATRS_COMBINE_EC_DATA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSmQbEns1l9v"
      },
      "source": [
        "# CSUMB EC PROCESSING STEP 1\n",
        "1.   Combine raw .dat files from towers\n",
        "\n",
        "Files exported from the eddy pro software are saved in the Ameriflux Standard output format as '.dat' files.  These files are capped at a certain size and need to be combined to generate a record comensurate with the entire period of observation.\n",
        "\n",
        "The script below reads in all 'raw' data for a given tower, *combines the data* into a large dataframe, *assigns a time index*, *removes duplicate rows* due to 'overlap' between '.dat' file records, *converts data values to numeric* from objects, and *saves the data to a csv* for evaluating quality, applying PI quality control flags, and gap-filling.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sciencebyAJ/watrs_ec_processing.git"
      ],
      "metadata": {
        "id": "UC5ifMFYrB6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI6V_J9S9c0E",
        "outputId": "0daece69-868e-4000-c7ea-e8a19266e05a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYFpGDFAhSzY"
      },
      "source": [
        "### Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "j0KBnMiYkObW"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "import requests\n",
        "from tables import NaturalNameWarning\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=NaturalNameWarning)\n",
        "verbose = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd oet_gf_ti/\n",
        "import src.combine_ec_data as combo\n"
      ],
      "metadata": {
        "id": "fohP5elNrc85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1ku1esEdKiq"
      },
      "source": [
        "### Set File Paths"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preserve for future use\n",
        "# meta_url = 'https://docs.google.com/spreadsheets/d/1cUHT0Rb0n39I0qk-bYY194spSWr7MNqkFX15PWnxXlI/edit?usp=sharing'\n",
        "# read_url = meta_url.replace('/edit?usp=sharing','/export?format=csv&gid=0')\n",
        "# meta_df = pd.read_csv(read_url)\n",
        "# covert to table above long term, use these for now\n",
        "# 2024-present fields"
      ],
      "metadata": {
        "id": "Wncjgm-SqiYy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dF2LhKnF4i0",
        "outputId": "22cf66de-a45a-42b2-d554-cd61a8c9a720"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/Shareddrives/WATRS_Field_Data/Field_Data/CSUMB_WineGrape_Zabala_2023/Data folder exists...\n",
            "\tcan not locate /content/drive/Shareddrives/WATRS_Field_Data/Field_Data/CSUMB_WineGrape_Zabala_2023/Combined_EC_Data/ ...\n",
            "\tmaking directory: /content/drive/Shareddrives/WATRS_Field_Data/Field_Data/CSUMB_WineGrape_Zabala_2023/Combined_EC_Data/ ... \n",
            "\tdirectory /content/drive/Shareddrives/WATRS_Field_Data/Field_Data/CSUMB_WineGrape_Zabala_2023/Combined_EC_Data/ created\n"
          ]
        }
      ],
      "source": [
        "LOGGERNET=False #<-- flag to address table format of Loggernet Pulled Tables vs Manually Pulled Tables\n",
        "\n",
        "lab_folder_path = '/content/drive/Shareddrives/WATRS_Field_Data/Field_Data/'\n",
        "raw_data_path = lab_folder_path+'CSUMB_WineGrape_Zabala_2023/' # will draw from metadata table\n",
        "\n",
        "field_name= combo.get_field_name(raw_data_path)\n",
        "# out paths\n",
        "out_path = raw_data_path+'Combined_EC_Data/'\n",
        "combo.check_folder(raw_data_path+'Data',out_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SJ0VH0fhfF1"
      },
      "source": [
        "### Get list of filenames for '.dat' Ameriflux formatted files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwNZcErcGS7U",
        "outputId": "b31695fa-5946-41ab-c3e3-fce4c0169ce2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 163 files in the folder\n"
          ]
        }
      ],
      "source": [
        "fnames = glob.glob(raw_data_path+'Data/*/1*AmeriFluxForma*.dat')\n",
        "\n",
        "if len(fnames)==0:\n",
        "  print('Check to see if data exists at:\\n'+raw_data_path)\n",
        "  print('Re-define raw-data path variable above')\n",
        "else:\n",
        "  print(f'There are {str(len(fnames))} files in the folder')\n",
        "fnames.sort(key=os.path.getmtime)\n",
        "\n",
        "i = 0\n",
        "for fname in fnames:\n",
        "  if i == 0:\n",
        "    df_i = pd.read_csv(fname)\n",
        "    col_list = list(df_i.columns)\n",
        "    df_all = df_i.copy()\n",
        "  else:\n",
        "    try:\n",
        "      if LOGGERNET == True:\n",
        "        df_i = pd.read_csv(LOGGERNETfname,header=1)\n",
        "      else:\n",
        "        df_i = pd.read_csv(fname)\n",
        "      df_i = combo.check_for_missing_columns(df_i,col_list)\n",
        "      df_all=pd.concat([df_all,df_i[col_list]],axis=0,ignore_index=True)\n",
        "      df_all.drop_duplicates(subset=['TIMESTAMP_END'],inplace=True)\n",
        "      if verbose == True:\n",
        "        print('x '+fname)\n",
        "    except:\n",
        "      try:\n",
        "        df_i = pd.read_csv(fname, encoding= 'unicode_escape',on_bad_lines='skip')\n",
        "        df_i = combo.check_for_missing_columns(df_i,col_list)\n",
        "        df_all=pd.concat([df_all,df_i[col_list]],axis=0,ignore_index=True)\n",
        "        df_all.drop_duplicates(subset=['TIMESTAMP_END'],inplace=True)\n",
        "        if verbose == True:\n",
        "            print('skipped bad lines\\nx '+fname)\n",
        "      except:\n",
        "        print('... not read '+fname)\n",
        "      pass\n",
        "  i+=1\n",
        "df_all.loc[df_all['TIMESTAMP_END']<2000, 'TIMESTAMP_END'] = np.nan\n",
        "df_all = df_all[df_all['TIMESTAMP_END'].notna()]\n",
        "if df_all.shape[0]==0:\n",
        "  print('check folder path for files/ncheckdatetime of df_all')\n",
        "\n",
        "df_time= combo.set_time_index(df_all)\n",
        "df_time.drop_duplicates(inplace=True)\n",
        "df_num = combo.to_numeric(df_time)\n",
        "todays_date_yyyymmdd=str(datetime.date.today().year)+str(datetime.date.today().month).zfill(2)+str(datetime.date.today().day).zfill(2)\n",
        "out_fname = out_path+field_name+'_'+todays_date_yyyymmdd+'.csv'\n",
        "df_num.to_csv(out_fname)\n",
        "\n",
        "print('file saved to: \\t'+out_fname)\n",
        "print('\\nThe processed data for '+ field_name + ' has '+ str(df_num.shape[0]/48 / 365.25)+' years of data')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "CYFpGDFAhSzY",
        "QVmkMc6NhWIq",
        "tKAsDS8edAzE"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}