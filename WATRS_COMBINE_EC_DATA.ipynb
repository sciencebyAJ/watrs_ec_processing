{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sciencebyAJ/watrs_ec_processing/blob/main/WATRS_COMBINE_EC_DATA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSmQbEns1l9v"
      },
      "source": [
        "# CSUMB EC PROCESSING STEP 1\n",
        "1.   Combine raw .dat files from towers\n",
        "\n",
        "Files exported from the eddy pro software are saved in the Ameriflux Standard output format as '.dat' files.  These files are capped at a certain size and need to be combined to generate a record comensurate with the entire period of observation.\n",
        "\n",
        "The script below reads in all 'raw' data for a given tower, *combines the data* into a large dataframe, *assigns a time index*, *removes duplicate rows* due to 'overlap' between '.dat' file records, *converts data values to numeric* from objects, and *saves the data to a csv* for evaluating quality, applying PI quality control flags, and gap-filling.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sciencebyAJ/watrs_ec_processing.git"
      ],
      "metadata": {
        "id": "UC5ifMFYrB6a",
        "outputId": "6fbfbb6c-06ae-4ee9-fd59-8c5d648e41dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'watrs_ec_processing'...\n",
            "remote: Enumerating objects: 23, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 23 (delta 3), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (23/23), 9.86 KiB | 2.46 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI6V_J9S9c0E",
        "outputId": "08602d71-c1a6-47a3-c646-434c98ea0502"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYFpGDFAhSzY"
      },
      "source": [
        "### Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "j0KBnMiYkObW"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "import requests\n",
        "from tables import NaturalNameWarning\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=NaturalNameWarning)\n",
        "verbose = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/watrs_ec_processing/"
      ],
      "metadata": {
        "id": "vvndZ7Yztdyr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import src.combine_ec_data as combo"
      ],
      "metadata": {
        "id": "BsTC0kMXtjgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preserve for future use\n",
        "# meta_url = 'https://docs.google.com/spreadsheets/d/1cUHT0Rb0n39I0qk-bYY194spSWr7MNqkFX15PWnxXlI/edit?usp=sharing'\n",
        "# read_url = meta_url.replace('/edit?usp=sharing','/export?format=csv&gid=0')\n",
        "# meta_df = pd.read_csv(read_url)\n",
        "# covert to table above long term, use these for now\n",
        "# 2024-present fields"
      ],
      "metadata": {
        "id": "k1Kik9i4uALJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1ku1esEdKiq"
      },
      "source": [
        "### Set File Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "9dF2LhKnF4i0",
        "outputId": "58e7566b-50ce-4546-f69e-bd2733a3f848"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-1d2e84fe7191>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# out paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mout_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_data_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'Combined_EC_Data/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mcombo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'Data'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/watrs_ec_processing/src/combine_ec_data.py\u001b[0m in \u001b[0;36mcheck_folder\u001b[0;34m(data_path, out_path)\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpresent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolders\u001b[0m \u001b[0mare\u001b[0m \u001b[0mcreated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   '''\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} folder exists...\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ],
      "source": [
        "LOGGERNET=False #<-- flag to address table format of Loggernet Pulled Tables vs Manually Pulled Tables\n",
        "\n",
        "lab_folder_path = '/content/drive/Shareddrives/WATRS_Field_Data/Field_Data/'\n",
        "raw_data_path = lab_folder_path+'CSUMB_WineGrape_Zabala_2023/' # will draw from metadata table\n",
        "\n",
        "field_name= combo.get_field_name(raw_data_path)\n",
        "# out paths\n",
        "out_path = raw_data_path+'Combined_EC_Data/'\n",
        "combo.check_folder(raw_data_path+'Data',out_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SJ0VH0fhfF1"
      },
      "source": [
        "### Combine all '{SITENAME}.dat' Ameriflux formatted files & Save Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwNZcErcGS7U",
        "outputId": "b31695fa-5946-41ab-c3e3-fce4c0169ce2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 163 files in the folder\n"
          ]
        }
      ],
      "source": [
        "fnames = glob.glob(raw_data_path+'Data/*/1*AmeriFluxForma*.dat')\n",
        "\n",
        "if len(fnames)==0:\n",
        "  print('Check to see if data exists at:\\n'+raw_data_path)\n",
        "  print('Re-define raw-data path variable above')\n",
        "else:\n",
        "  print(f'There are {str(len(fnames))} files in the folder')\n",
        "fnames.sort(key=os.path.getmtime)\n",
        "\n",
        "i = 0\n",
        "for fname in fnames:\n",
        "  if i == 0:\n",
        "    df_i = pd.read_csv(fname)\n",
        "    col_list = list(df_i.columns)\n",
        "    df_all = df_i.copy()\n",
        "  else:\n",
        "    try:\n",
        "      if LOGGERNET == True:\n",
        "        df_i = pd.read_csv(LOGGERNETfname,header=1)\n",
        "      else:\n",
        "        df_i = pd.read_csv(fname)\n",
        "      df_i = combo.check_for_missing_columns(df_i,col_list)\n",
        "      df_all=pd.concat([df_all,df_i[col_list]],axis=0,ignore_index=True)\n",
        "      df_all.drop_duplicates(subset=['TIMESTAMP_END'],inplace=True)\n",
        "      if verbose == True:\n",
        "        print('x '+fname)\n",
        "    except:\n",
        "      try:\n",
        "        df_i = pd.read_csv(fname, encoding= 'unicode_escape',on_bad_lines='skip')\n",
        "        df_i = combo.check_for_missing_columns(df_i,col_list)\n",
        "        df_all=pd.concat([df_all,df_i[col_list]],axis=0,ignore_index=True)\n",
        "        df_all.drop_duplicates(subset=['TIMESTAMP_END'],inplace=True)\n",
        "        if verbose == True:\n",
        "            print('skipped bad lines\\nx '+fname)\n",
        "      except:\n",
        "        print('... not read '+fname)\n",
        "      pass\n",
        "  i+=1\n",
        "df_all.loc[df_all['TIMESTAMP_END']<2000, 'TIMESTAMP_END'] = np.nan\n",
        "df_all = df_all[df_all['TIMESTAMP_END'].notna()]\n",
        "if df_all.shape[0]==0:\n",
        "  print('check folder path for files/ncheckdatetime of df_all')\n",
        "\n",
        "df_time= combo.set_time_index(df_all)\n",
        "df_time.drop_duplicates(inplace=True)\n",
        "df_num = combo.to_numeric(df_time)\n",
        "todays_date_yyyymmdd=str(datetime.date.today().year)+str(datetime.date.today().month).zfill(2)+str(datetime.date.today().day).zfill(2)\n",
        "out_fname = out_path+field_name+'_'+todays_date_yyyymmdd+'.csv'\n",
        "df_num.to_csv(out_fname)\n",
        "\n",
        "print('file saved to: \\t'+out_fname)\n",
        "print('\\nThe processed data for '+ field_name + ' has '+ str(df_num.shape[0]/48 / 365.25)+' years of data')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "CYFpGDFAhSzY",
        "QVmkMc6NhWIq",
        "tKAsDS8edAzE"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}